{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This report highlights the wrangling efforts that went into cleaning the three data sets: `twitter-archive-enhanced.csv`,`image_predictions.tsv` and `additional_data.csv`. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017. The wranlging process consists of gathering, assessing and cleaning.\n",
    "\n",
    "### 1. Data gathering\n",
    "\n",
    "To begin, the `twitter_archive_enhanced.csv` dataset was downloaded from the WeRateDogs twitter archive.\n",
    "\n",
    "Secondly, the `image_predictions.tsv` dataset hosted on Udacityâ€™s servers was downloaded using the request python library. It contains a neural network's prediction of  what breed of dog is present in each tweet.\n",
    "___\n",
    "```python\n",
    "url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'\n",
    "image_predictions = requests.get(url)\n",
    "```\n",
    "___\n",
    "\n",
    "and written to a file named `image_prediction.tsv`\n",
    "___\n",
    "```python\n",
    "with open(url.split('/')[-1].replace('-','_'),mode='wb') as file:\n",
    "    file.write(image_predictions.content)\n",
    "```\n",
    "___\n",
    "Lastly, the `additional_data.csv` dataset was queried from twitter API using tweepy pythonlibrary. To achieve this, twiter keys and tokens were used to authorize the twitter api.\n",
    "___\n",
    "\n",
    "```python\n",
    "import tweepy\n",
    "consumer_key    = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "consumer_secret = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_token    = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "access_secret   = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "```\n",
    "___\n",
    "\n",
    "The tweets are queried and dumped into a `tweet_jsons.txt` file.\n",
    "''' \n",
    "___\n",
    "```python\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "with open('tweet_jsons.txt','w') as file:\n",
    "    for tweet_id in df_tweet1.tweet_id:\n",
    "        start=time.time()\n",
    "        try:\n",
    "            tweet=api.get_status(tweet_id,tweet_mode='extended')\n",
    "            json.dump(tweet._json,file)\n",
    "            file.write('\\n')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        print('tweet_id: {} \\t Run time: {}'.\\\n",
    "              format(tweet_id,time.time() -start))\n",
    "```\n",
    "___\n",
    "After which the tweet id, retweet count and favorite count was mined and saved into a DataFrame which is then saved into a file called `additional_data.csv`\n",
    "___\n",
    "```python\n",
    "df_list = []\n",
    "with open('tweet_jsons.txt','r') as file:\n",
    "    for lines in file.readlines():\n",
    "        line = json.loads(lines)\n",
    "        tweet_id = line[\"id\"]\n",
    "        retweet_count = line[\"retweet_count\"]\n",
    "        favorite_count = line[\"favorite_count\"]\n",
    "\n",
    "        df_list.append({\n",
    "            'tweet_id': tweet_id,\n",
    "            'retweet_count': retweet_count,\n",
    "            'favorite_count': favorite_count\n",
    "        })\n",
    "df = pd.DataFrame(df_list,columns['tweet_id',\n",
    "                                  'retweet_count',\n",
    "                                  'favorite_count'])\n",
    "df.to_csv('additional_data.csv',index=False)\n",
    "```\n",
    "___\n",
    "\n",
    "### 2. Asessing data\n",
    "The gathered data as visually assessed using Microsoft excel and programmatically assessed for quality (dirty) and tidiness (messy) issues. Dirty data has issues with its contentin areas such as completeness, validity, accuracy, and consistency.\n",
    "While untidy data has issues with its structure. For data to be tidy:\n",
    "1. Each variable forms a column.\n",
    "2. Each observation forms a row.\n",
    "3. Each type of observational unit forms a table.\n",
    "\n",
    "THe following quality and tidiness issues were identified for all three datasets:\n",
    "\n",
    "#### Quality issues\n",
    "\n",
    "> ##### Twitter archives\n",
    "\n",
    "1. `text` column contains 3 variables text, ratings and Urls instead of only text\n",
    "\n",
    "2. In_reply_to_status_id, in_reply_to_user_id table contain to many missing values and of little value\n",
    "\n",
    "3. Some entires are retweets as shown in `retweeted_status_id` column \n",
    "\n",
    "4. `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp` columns are not useful once retweet entries are droped\n",
    "\n",
    "5. Incorrect data type of `timestamp` column in string instead of datetime\n",
    "\n",
    "6. `expanded_urls` column contain duplicated values\n",
    "\n",
    "7. `source` column contains extraneous \n",
    "\n",
    "8. tweet_id are in different format and thus cannot be used as merge criteria\n",
    "\n",
    "9. `name` column contain inconsistent name and 'None' format\n",
    "\n",
    "> ##### Image prediction\n",
    "\n",
    "1. Some entries in `p1_dog`, `p2_dog`, `p3_dog` columns table are not dog ratings\n",
    "\n",
    "2. `p1`, `p2`, `p3` columns name are not decriptive \n",
    "\n",
    "3. Dog names are not consistent with some having small and others large caps \n",
    "\n",
    "4. `p1`, `p2`, `p3` columns in image prediction table contian inconsistent format for dog name, some with capitals whilst some with small caps \n",
    "\n",
    "5. tweet_id are indifferent format and thus cannot be used as merge criteria\n",
    "\n",
    "> ##### additional data\n",
    "\n",
    "1. tweet_id are indifferent format and thus cannot be used as merge criteria\n",
    "\n",
    "#### Tidiness issues\n",
    "> #### twitter achive\n",
    "\n",
    "1.  `Doggo`,`floofer`, `pupper`, `puppo` in twitter archive table are one variable yet they are in separate columns\n",
    "\n",
    "2. Contain observations beyoung beyond August 1st, 2017\n",
    "\n",
    "> ##### additional data\n",
    "\n",
    "1. additional data table should be merged of twitter archive and image prediction\n",
    "\n",
    "### 3. Cleaning data\n",
    "The quality and tidiness issues identified in the assessing stage are cleaned programmatically using the define, code and test process.\n",
    "\n",
    "#### 3.1.0. quality issues\n",
    "#### 3.1.1. twitter archives\n",
    "##### Define: separate and remove ratings and urls from text as they are already present in adjacent columns\n",
    "```python\n",
    "tweet_arch_copy.text = pd.Series(tweet_arch_copy.text.\\\n",
    "          apply(lambda x:x.split('/10')[0][:-3]))\n",
    "```\n",
    "#### Define: drop columns: in_reply_to_status_id, in_reply_to_user_id in twitter archive  table as they are of little relevenace\n",
    "```python\n",
    "drop_col = ['in_reply_to_status_id', 'in_reply_to_user_id']\n",
    "tweet_arch_copy.drop(drop_col,axis=1,inplace=True)\n",
    "```\n",
    "#### Define: filter out retweet entries and drop retweeted_status_id columns\n",
    "```python\n",
    "tweet_arch_copy = tweet_arch_copy[tweet_arch_copy.retweeted_status_id.isnull()]\n",
    "```\n",
    "#### Define:  drop retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp \n",
    "```python\n",
    "drop_cols = ['retweeted_status_id',\n",
    "             'retweeted_status_user_id',\n",
    "             'retweeted_status_timestamp']\n",
    "tweet_arch_copy.drop(drop_cols,axis=1,inplace=True)\n",
    "```\n",
    "#### Define: remove repeated ULRs\n",
    "```python\n",
    "tweet_arch_copy.drop_duplicates(subset='expanded_urls',keep='first')\n",
    "```\n",
    "#### Define: drop extraneous details from source \n",
    "```python\n",
    "tweet_arch_copy.source = tweet_arch_copy.source\\\n",
    "                          .apply(lambda x:soup (x,'lxml')\\\n",
    "                          .contents[0].body.a.contents[0])\n",
    "```\n",
    "#### Define: convert tweet_id to int64 \n",
    "```python \n",
    "tweet_arch_copy.tweet_id = tweet_arch_copy.tweet_id.astype(np.int64)\n",
    "```\n",
    "#### Define: convert name format to lower caps and replace None with np.nan \n",
    "```python\n",
    "tweet_arch_copy.name = tweet_arch_copy.name.str.lower()\n",
    "tweet_arch_copy.name.replace('none',np.nan,inplace=True)\n",
    "```\n",
    "#### 3.1.2. Image prediction\n",
    "#### Define: filter out non-dog entries with booloean using `p1_dog`, `p2_dog`, `p3_dog` columns and drop those columns afterwards\n",
    "```python\n",
    "pred_drop = ['p1_dog','p2_dog','p3_dog']\n",
    "img_pred_copy = img_pred_copy.query('p1_dog==True')\\\n",
    "                             .drop(pred_drop,axis=1)\n",
    "```\n",
    "#### Define: rename colums with more descriptive names\n",
    "```python\n",
    "replaced = {'p1':'first_class_prediction','p1_conf':'first_class_prediction_confidence',\\\n",
    "            'p2':'second_class_prediction','p2_conf':'second_class_prediction_confidence',\\\n",
    "            'p3':'third_class_prediction','p3_conf':'third_class_prediction_confidence'}\n",
    "img_pred_copy.rename(columns=replaced ,inplace=True)\n",
    "```\n",
    "#### Define: `p1`, `p2`, `p3` columns in image prediction table contian inconsistent format for dog name, some with capitals ,others with small caps. change to lower caps\n",
    "```python\n",
    "img_pred_copy.first_class_prediction = img_pred_copy\\\n",
    "                                        .first_class_prediction\\\n",
    "                                        .str.replace('-','_')\\\n",
    "                                        .str.replace('_',' ')\\\n",
    "                                        .str.lower()\n",
    "\n",
    "img_pred_copy.second_class_prediction = img_pred_copy\\\n",
    "                                        .second_class_prediction\\\n",
    "                                        .str.replace('-','_')\\\n",
    "                                        .str.replace('_',' ')\\\n",
    "                                        .str.lower()\n",
    "\n",
    "img_pred_copy.third_class_prediction = img_pred_copy\\\n",
    "                                        .third_class_prediction\\\n",
    "                                        .str.replace('-','_')\\\n",
    "                                        .str.replace('_',' ')\\\n",
    "                                        .str.lower()\n",
    "```\n",
    "#### Define: convert tweet_id to int\n",
    "```python\n",
    "img_pred_copy.tweet_id = img_pred_copy.tweet_id.astype(np.int64)\n",
    "```\n",
    "#### 3.1.3. Additional data\n",
    "#### Define: convert tweet_id to int\n",
    "```python\n",
    "add_data_copy.tweet_id = add_data_copy.tweet_id.astype(np.int64)```\n",
    "\n",
    "### 3.2.0. tidiness issues cleanup\n",
    "#### 3.2.1. twitter achives\n",
    "#### Define: concatenate 'doggo','pupper', 'floofer', 'puppo' columns into one column called stage\n",
    "```python\n",
    "\n",
    "stage = ['doggo','pupper', 'floofer', 'puppo' ]\n",
    "for i in stage:\n",
    "    tweet_arch_copy[i] = tweet_arch_copy[i].replace('None', '')\n",
    "    \n",
    "tweet_arch_copy['stage'] = tweet_arch_copy.doggo.str.\\\n",
    "                           cat(tweet_arch_copy.floofer).str.\\\n",
    "                           cat(tweet_arch_copy.pupper).str.\\\n",
    "                           cat(tweet_arch_copy.puppo)\n",
    "\n",
    "tweet_arch_copy.drop(columns=stage,axis=1,inplace=True)\n",
    "```\n",
    "\n",
    "#### Define: filter entries below Auhust, 1st, 2017\n",
    "```python\n",
    "tweet_arch_copy = tweet_arch_copy.query('timestamp < \"2017-08-01\"')```\n",
    "\n",
    "#### 3.2.2. Additional data \n",
    "#### Define: merge additional data, image prediction and twitter archive \n",
    "```python\n",
    "twitter_archive_master = tweet_arch_copy\\\n",
    "                        .merge(add_data_copy,on='tweet_id')\\\n",
    "                        .merge(img_pred_copy,on='tweet_id')```\n",
    "                        \n",
    "### 4. Storing Data\n",
    "Save gathered, assessed, and cleaned master dataset to a CSV file named `\"twitter_archive_master.csv\"`.\n",
    "```python\n",
    "twitter_archive_master.to_csv('twitter_archive_masterr.csv')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
